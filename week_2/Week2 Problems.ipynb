{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly Tutorial Problems: Week 2\n",
    "Jacob Sanz-Robinson, Meenakshi Khosla, Eric Bridgeford\n",
    "\n",
    "Welcome to the second set of tutorial problems of the ABCD ReproNim Machine Learning course!\n",
    "\n",
    "Let's get started. If you aren't familiar with any of the modules we are importing for a problem, try skimming through the module documentation for hints and examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1: Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Linear SVM\n",
    "\n",
    "We have loaded up sklearn's Iris dataset (as well as a few other sklearn modules) and wish to use a model to classify it's 3 species of flowers.\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "* Using sklearn, create a pipeline that uses the StandardScaler to remove the mean and scale the data to unit variance, and then uses the LinearSVC to implement an SVM. The SVM should have hyperparameters: C=10, and hinge loss function. Fit the pipeline to the data, and cross-validate it.\n",
    "* If you wanted a non-linear kernel, you would use the \"sklearn.svm.SVC\" module instead. Have a brief glance at it's documentation and the different types of kernels you can specify as parameters (it's good to know these exist!). Optionally, implement a \"sklearn.svm.SVC\" model for the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = iris[\"target\"].astype(np.float64) #3 classes of flowers.\n",
    "\n",
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Tree-based model and ensembles: California housing dataset regression\n",
    "\n",
    "\n",
    "We are going to be using our new ML knowledge to try and estimate median house values in Californian housing districts using some tree-based models. The [sklearn dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset) we will use  was derived from the 1990 U.S. census. The prices are in hundreds of thousands of dollars ($100,000).\n",
    "\n",
    "Firstly, we need to import the libraries we will be using, and download the data. If you don't recognize a module we are importing, I suggest you have a quick glance at it's documentation page!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "import pandas as pd\n",
    "\n",
    "chd = datasets.fetch_california_housing()\n",
    "X, y = chd[\"data\"], chd[\"target\"]\n",
    "\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"y shape: \", y.shape)\n",
    "print(\"\\n Data X\")\n",
    "print(X)\n",
    "print(\"\\n Target y:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task:**\n",
    "* Build a DecisionTreeRegressor and a RandomForestRegressor and cross-validate each of them on the housing data. Start by using a maximum depth of 3 for both trees, and 10 estimators for the Random Forest.\n",
    "* Try a few different settings for the parameters (or do a grid search). Report the parameters that led to the best scores.\n",
    "* Use VotingRegressor to make an ensemble that averages the output of the two tree models. Try to make the ensemble outperform all of the base models. If necessary add more models of your choice to the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Wine Classification\n",
    "\n",
    "Since I figure it's a good assumption that most of the students reading this [enjoy some wine](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6072905/) from time to time, this problem is about classifying different kinds of the classic fermented grape beverage. Any typos found in this question are purely incidental. \n",
    "\n",
    "I based this question on one of [Microsoft's Documentation ML Basics GitHub repo](https://github.com/MicrosoftDocs/ml-basics)'s challenges, and I highly suggest you check out their repository if you want more ML problems with solutions when you finish this notebook!\n",
    "\n",
    "We import the dataset and inspect it's contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "wine = load_wine(as_frame=True)\n",
    "X, y = wine['data'].values, wine['target'].values\n",
    "\n",
    "#See features for 10 random wines\n",
    "data = pd.DataFrame(data=np.c_[wine['data'],wine['target']],columns=wine['feature_names']+['target'])\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See dimensions\n",
    "print(\"X shape: \" + str(X.shape))\n",
    "print(\"y shape: \" + str(y.shape))\n",
    "\n",
    "#See features and labels for first 4 wines\n",
    "for n in range(0, 4):\n",
    "    print(\"Wine\", str(n+1), \"\\n  Features:\",list(X[n]), \"\\n  Label:\", y[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task:**\n",
    "\n",
    "* Write a wine classifier and cross-validate it! Try to obtain upwards of 80% accuracy. Get creative and use any supervised learning model from the lecture, and any preprocessing steps you fancy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task:**\n",
    "\n",
    "* Use the VotingClassifier module to make a classification ensemble that uses hard voting. Add at least two more models to the ensemble alongside the one from the last task, and try to obtain a higher score in your cross-validation than you initially did with a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Unsupervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: K-means Clustering\n",
    "\n",
    "In this question we are going to have a look at K-Means clustering. Firstly we generate two sets of 2D data. We generate isotropic Gaussian blobs using make_blobs, and interleaving half circles using make_moons.\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "* Build and fit a k-means models for each of the two datasets we synthesized. Use 3 clusters for the blobs, and 2 for the moons.\n",
    "* Obtain the cluster labels or predictions for each of the datasets, and plot them (preferably each cluster should correspond to a different colour or marker).\n",
    "* On which of the datasets does K-means clustering perform better? Why could this be the case?\n",
    "* Read the [sklearn documentation for K-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). Be aware that parameters like n_init, and max_iter exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_blobs(random_state=1)\n",
    "X_2, y_2 = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part is more of an example for your learning than a problem for you to solve.\n",
    "What if we had a dataset where we didn't know how many clusters we wanted? Let's turn out attention back to the blobs. We would use the elbow method, as follows. We run the clustering for a range of clusters k (we are using 1 to 10 for this example) and for each value, we calculate the sum of squared distances from each point to its assigned center, known as distortions. When the distortions are plotted against k, we look for the k where the line looks like a sharp “elbow” (the point of inflection on the curve). This will be our best value of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_blobs(random_state=5)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=60)\n",
    "\n",
    "distortions = []\n",
    "K = range(1,10)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k)\n",
    "    kmeanModel.fit(X)\n",
    "    distortions.append(kmeanModel.inertia_)\n",
    "    \n",
    "#Plotting the distortions of K-Means\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method for finding the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Gaussian Mixture Models, Generating Data, and Embeddings\n",
    "\n",
    "We start off with a similar setup to our 2-moons dataset task in the previous problem.\n",
    "\n",
    "**Your task:**\n",
    "* Create a GaussianMixture model with 2 components and choose the adequate parameter to ensure each component has its own general covariance matrix (see the documentation for the parameter you need).\n",
    "* Fit the model to the data and predict the classes of the moon data points. Plot your predictions, did you notice any improvements over k-means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point some of you will be wondering \"So...How can we succesfully separate the two strips of the two moons dataset?\". One technique that is often attempted for such tasks are the Spectral Embeddings we saw in class.\n",
    "\n",
    "**Your task:**\n",
    "* Recreate your GMM clustering solution in the past task of the question. However, before the GMM model, use a SpectralEmbedding with 2 components to transform the data. Feed this transformed version of the data to the GMM and plot the GMM's predictions. How does it perform now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the added bonuses of using a GMM, then? While they do often perform better on more complex-shaped clustering situations than k-means, GMMs are technically not clustering models, but rather probability-based density estimation models. They have the ability to describe the way in which data is distributed, and generate it.\n",
    "\n",
    "**Your task:**\n",
    "* Say you want to model the dataset distribution and generate your own moon shapes similar to the data we fitted.\n",
    "* Firstly, Create another GMM and fit it to the data, this time make it have 16 components. Using 16 Gaussians allows us to model the overall distribution of the input data, rather than finding clusters.\n",
    "* Secondly, Use the 'sample' method from the GMM to generate 200 data points.\n",
    "* Plot these points, what do they look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Applying these concepts to Neuroscience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final problem has the goal of showing you how the discussed unsupervised learning techniques can be applied to real neuroimaging data. We have provided a MatLab (.mat) file containing a single diffusion connectome (in a few different atlases) acquired using a parcellation that has hemispheric separation of nodes.\n",
    "[This paper](https://www.pnas.org/content/116/13/5995) found that  left/right hemisphere community separation can be achieved with unsupervised learning methods. We will implement a simplified version of this pipeline.\n",
    "\n",
    "**Your task:**\n",
    "* Use a Spectral Embedding to transform the connectivity data. Use 2 components for visualization purposes.\n",
    "* Using a relevant unsupervised learning technique from the lecture, perform clustering on the transformed data. Make a plot to visualize your model's predictions.\n",
    "* OPTIONAL: If anybody wants to read the paper and try implementing more of their details into your code and see how it affects the results...We'd be curious to hear how it went!\n",
    "\n",
    "It *should* split by hemisphere, but due to the simplifications it could instead split functionally (i.e. Yeo groups) depending on how coarse the parameters want the clusters to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "#Read in the connectome MAT file\n",
    "mat = scipy.io.loadmat('acq-64dir_space-T1w_desc-preproc_space-T1w_msmtconnectome.mat')\n",
    "\n",
    "#Obtain the streamline count weighted by both SIFT and inverse node volumes\n",
    "#Using AAL coz it is anatomically defined, as opposed to functionally defined like most other atlases in the file.\n",
    "connectivity = mat[\"aal116_sift_invnodevol_radius2_count_connectivity\"]\n",
    "\n",
    "con = np.asarray(connectivity)\n",
    "print(con.shape)\n",
    "\n",
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
