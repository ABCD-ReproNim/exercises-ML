{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Introduction to deep learning\n",
    "\n",
    "**Created**: 03/10/2022  \n",
    "**Author**: Gareth Harman  \n",
    "**Entity**: ReproNim \n",
    "\n",
    "### Module Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Data prep and helpers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm \n",
    "import os\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "# Additional helper functions\n",
    "import week_4_helperFx as fx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets create some challenging data to model\n",
    "\n",
    "- Here we are going to generate spiral data \n",
    "- We will have two classes (binary classfication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOU CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''##################################################################################\n",
    "- Play around with generating this spiral data\n",
    "- How difficult might these decision boundaries be to classify?\n",
    "- How much noise before you feel YOU would struggle to draw a clean boundary line?\n",
    "##################################################################################'''\n",
    "\n",
    "\n",
    "# Generate some data and return the pandas dataframe\n",
    "num_obs = 1000\n",
    "num_spiral = _\n",
    "noise_amt = _\n",
    "\n",
    "_, _, _ = fx.generateSpiral(_, _, _)\n",
    "\n",
    "# Plot the data and use color to view classes\n",
    "fig, ax = plt.subplots(figsize = (7, 7))\n",
    "sns.scatterplot(x = 'x', y = 'y', hue = 'class', palette = 'coolwarm', data = _, s = 100, alpha = .9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch: Working with data\n",
    "\n",
    "### PyTorch Tensors\n",
    "\n",
    "- It is likely most of us are familiar with using numpy arrays \n",
    "- PyTorch requires the use of `torch.tensors`\n",
    "- **Both of these objects are matrices**\n",
    "- But `torch.tensors` have two important attributes\n",
    "\n",
    "**`.requires_grad`**\n",
    "- If `True`, this attribute lets us know whether we should keep track and calculate gradients for this tensor\n",
    "\n",
    "**`.to()`**\n",
    "- This function allows us to send objects to different devices (i.e., CPU or GPU)\n",
    "- Additionally, we can use this function to cast a tensor to a data type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOU CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a random numpy array\n",
    "z = np.random.rand(50, 50)\n",
    "\n",
    "# Recast this to a tensor\n",
    "z = _(_, dtype = _)\n",
    "\n",
    "# Check if this tensor is going to keep track of the gradient\n",
    "print(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## PyTorch Dataset\n",
    "\n",
    "Adapted from pytorch's documentation [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n",
    "```python\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "```\n",
    "\n",
    "From the pytorch documentation \n",
    "\n",
    "> The `Dataset` object retrieves our datasetâ€™s features and labels one sample at a time\n",
    "\n",
    "\n",
    "- Many existing datasets such as `MNIST` or `Fashion-MNIST` have functions for downloading and loading\n",
    "- However, more commonly we will want to create our own **custom** dataset\n",
    "- To create our own dataset class we are required to include the following three class methods \n",
    "\n",
    "```python\n",
    "class MyDataset:\n",
    "```\n",
    "\n",
    "```python\n",
    "\n",
    "    __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "```\n",
    "\n",
    "- This function is just the normal python class initialization method\n",
    "- In this exercise we are feeding our data in directly, but another common scenario may be reading in some .csv that contains columns for `subjectid`, `outcome`, and `image_path` \n",
    "\n",
    "```python\n",
    "\n",
    "    __len__(self):\n",
    "        return len(self.y)\n",
    "        \n",
    "```\n",
    "\n",
    "- This function's sole duty is to return the number of observations in the dataset\n",
    "\n",
    "```python\n",
    "\n",
    "    __getitem___(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "        \n",
    "```\n",
    "\n",
    "- This function is the bulk of the custom dataset class\n",
    "- This method simply obtains a single observation from our data given an index \n",
    "- While this logic is required in all `__getitem__` methods, they typically contain additional elements\n",
    "- For example, with imaging data this function may simply index a specific row in our earlier mentioned .csv to get our `subjectid`, `outcome`, and `image_path`\n",
    "- We would then also load our imaging data given the `image_path` \n",
    "- Additionally we could apply some other operations (such as min-max scaling) in here\n",
    "\n",
    "**Note**:\n",
    "\n",
    "- This method is called **MANY** times during the training of our model so if we are doing additional operations we may want to keep them minimal\n",
    "- We can use ptyhon's `multiprocessing` to help speed this up\n",
    "- Additionally, when working with niftis I have found that using another script to load and then saving the niftis as `torch.tensors` to be much more efficient. The `torch.load()` is much faster than say `nibabel.load()`\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOU CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''##################################################################################\n",
    "- Create a custom dataset class called MyDataset\n",
    "- Remember, it must conatin __init__(), __len__(), and __getitem__() methods\n",
    "##################################################################################'''\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ):  \n",
    "        \n",
    "        # Load our data as class attributes\n",
    "        \n",
    "        # With pytorch we have to work with torch.tensors so make sure to recasst\n",
    "        \n",
    "    \n",
    "    def __len__(self): \n",
    "        \n",
    "        # Return the number of observations in our dataset\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, ): \n",
    "        \n",
    "        # Return a single observation of X and y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## DataLoaders\n",
    "\n",
    "- The **reason** why we create the pytorch Dataset\n",
    "- This object allows us to easily iterate through batches of our data to train our model\n",
    "\n",
    "Again, from the pytorch documentation\n",
    "\n",
    "> `DataLoader` is an iterable that abstracts this complexity for us in an easy API.\n",
    "\n",
    "- Most frequently we update our weights in batches, both to introduce stochaisticity and overcome memory limitations \n",
    "- PyTorch has a built in data object called a `DataLoader`\n",
    "\n",
    "If youre at all planning to use the dataloader I highly recommend reading the documentation [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOU CODE BELOW\n",
    "\n",
    "1. Create a new batch of data with the given parameters\n",
    "2. Split the data into train/test sets\n",
    "3. Create the train/test Datasets using our custom `MyDataset` class\n",
    "4. Create the `DataLoader` object using our custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate some spiral data\n",
    "X, y, df = fx.generateSpiral(750, 4, noise = 1.25)\n",
    "\n",
    "# Split into train and test\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(_, _, test_size=0.33, random_state=42)\n",
    "\n",
    "# We first create datasets of our custom dataset class for train/test\n",
    "tr_data = MyDataset(_, _)\n",
    "te_data = MyDataset(_, _)\n",
    "\n",
    "# Now create the instances of the train/test loaders\n",
    "tr_loader = DataLoader(_, batch_size = 50, shuffle = True)\n",
    "te_loader = DataLoader(_, batch_size = 50, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch nn.Module and Sequential\n",
    "\n",
    "Below we will create a simple 4 layer neural network\n",
    "\n",
    "### Creating the network: Option 1 (manual)\n",
    "\n",
    "- With this option we must explicitly define our layers and order of operations in our forward pass\n",
    "- Additionally we **must** define a `forward()` call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NetOpt1(nn.Module):\n",
    "    \n",
    "    def __init__(self, act):\n",
    "        \n",
    "        nn.Module.__init__(self)\n",
    "        \n",
    "        # The activation function of our choosing\n",
    "        self.act = act\n",
    "        \n",
    "        # Network instantiation\n",
    "        self.fc1 = nn.Linear(2, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sig(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the network: Option 2 (with sequential)\n",
    "\n",
    "- Sequential is nice because it assumes our data moves **in order** (as specified from the sequential)\n",
    "- Additionally, we do not need to define the `forward()` function because it is assumed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, act):\n",
    "        \n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.act = act\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            \n",
    "                            # Layer 1\n",
    "                            nn.Linear(2, 10),\n",
    "                            self.act,\n",
    "\n",
    "                            # Layer 2\n",
    "                            nn.Linear(10, 10),\n",
    "                            self.act,\n",
    "\n",
    "                            # Layer 3\n",
    "                            nn.Linear(10, 1),\n",
    "                            nn.Sigmoid()\n",
    "\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we wanted create a neural network for a regression problem\n",
    "\n",
    "- We would drop the last activation function (sigmoid) \n",
    "- And use MSELoss \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the main function train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(train_loader, test_loader, n, criterion, lr = .001, n_epochs = 100):\n",
    "\n",
    "    '''\n",
    "    This function is our wrapper to create an instance and train our model \n",
    "    Note: Typically I wouldnt recommend cramming all of this in a function but for ease of running multiple \n",
    "        different parameters here we are\n",
    "        \n",
    "    Attributes:\n",
    "        train_loader (DataLoader): The instance of our training dataloader\n",
    "        test_loader  (DataLoader): The instance of our test dataloader\n",
    "        n            (nn.Sequential): Our network object\n",
    "        criterion    (DataLoader): Our loss function\n",
    "        lr           (Float): Our learning rate for our otpimizer\n",
    "        n_epochs     (Int): The number of iterations to train for\n",
    "        act          (nn.Functional): The activation function from pytorch functional\n",
    "        \n",
    "    Returns:\n",
    "        net          Our trained network object for boundary plotting\n",
    "        perf         The performance of our model\n",
    "    '''\n",
    "    \n",
    "    # Store our loss both our training and test data\n",
    "    perf = {'loss': [], 'type': [], 'epoch': []}\n",
    "\n",
    "    # Set our optimizer\n",
    "    optimizer = optim.Adam(n.parameters(), lr = lr)\n",
    "\n",
    "    \n",
    "    '''###############################################\n",
    "    Training\n",
    "    ###############################################'''\n",
    "    \n",
    "    for epoch in range(n_epochs): \n",
    "        \n",
    "        train_loss = []; test_loss =[]          # Store loss at each epoch\n",
    "\n",
    "        for batch_idx, (train_x, train_y) in enumerate(tr_loader):\n",
    "\n",
    "            optimizer.zero_grad()               # Zero out the gradient\n",
    "            tr_pred = n.model.forward(train_x)      # Our forward pass\n",
    "\n",
    "            # Calculate and store loss\n",
    "            tr_loss = criterion(tr_pred.squeeze(), train_y.squeeze())\n",
    "            train_loss.append(tr_loss.item())\n",
    "            \n",
    "            tr_loss.backward()                  # Compute our gradients\n",
    "            optimizer.step()                    # Step on our loss surface\n",
    "            \n",
    "            \n",
    "        '''###############################################\n",
    "        Test Evaluation\n",
    "        ###############################################'''\n",
    "    \n",
    "        for batch_idx, (test_x, test_y) in enumerate(te_loader):\n",
    "\n",
    "            with torch.no_grad():               # Dont store the gradient\n",
    "\n",
    "                # Forward, calculate loss and store\n",
    "                test_pred = n.model.forward(test_x)\n",
    "                te_loss = criterion(test_pred.squeeze(), test_y.squeeze())\n",
    "                test_loss.append(te_loss.item())\n",
    "\n",
    "        # Update performance metrics\n",
    "        perf = fx.update_perf(perf, train_loss, test_loss, epoch)\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(f'| Epoch: {str(epoch).zfill(3)} | Train Loss: {tr_loss.item():.4f} | Test Loss: {te_loss.item():.4f} |')\n",
    "    \n",
    "    return n.model, pd.DataFrame(perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## What is happening?\n",
    "\n",
    "---\n",
    "\n",
    "### Zeroing the gradient\n",
    "\n",
    "**PyTorch Documentation**: [here](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html)\n",
    "\n",
    "```python line_num=True\n",
    " optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "- We set our gradient to zero\n",
    "\n",
    "### Forward pass\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "tr_pred = n.model.forward(tr_xx)\n",
    "```\n",
    "\n",
    "- This is our forward pass \n",
    "- Data moves through our network and we get some prediction\n",
    "\n",
    "### Weight updating\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "tr_loss.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "- Here we are calling `backward()` which propagates our loss **backward** through our network\n",
    "- The `step` call allows us to, well, *step* towards some potential minima on our loss surface\n",
    "\n",
    "### Test evaluation\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "```\n",
    "\n",
    "- When evaluating our test data, it is not necessary to store or keep track of the gradient in any way\n",
    "- These operations can be expensive both in terms of memory and computation\n",
    "- With no grad is a nice easy way to set all objects with the `requires_grad` attribute to `False`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN THE NETWORK\n",
    "\n",
    "## Lets set up the parameters for our network\n",
    "\n",
    "1. Pick an activation function from [here](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n",
    "2. Create an instance of the network\n",
    "3. Set the learning rate\n",
    "4. Select our loss function from [here](https://pytorch.org/docs/stable/nn.html#loss-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOU CODE HERE\n",
    "\n",
    "Try the following configurations and check to see if the network will converge \n",
    "\n",
    "**Also try and consider why/why not this network might be converging**\n",
    "\n",
    "Play around with different `activation fx`, `learning rate`, `number of epochs`\n",
    "\n",
    "**First**\n",
    "- Activation Function: `ReLU`\n",
    "- Learning Rate: `.00001`, `1`\n",
    "- Epochs: `250`\n",
    "\n",
    "**Second**\n",
    "- Activation Function: `Sigmoid`\n",
    "- Learning Rate: `.01`\n",
    "- Epochs: `250`\n",
    "\n",
    "**Third**\n",
    "- Activation Function: `ReLU`\n",
    "- Learning Rate: `.01`\n",
    "- Epochs: `250`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The parameters we can tune\n",
    "act = nn._() # Set activation function\n",
    "lr = _ # Set the learning rate\n",
    "n_epochs = _ # How long to train for\n",
    "\n",
    "n = Net(act) # Create an instance of our networks\n",
    "criterion = nn._() # Our loss function\n",
    "\n",
    "net, perf = train_model(_, _, _, _, lr = _, n_epochs = _)\n",
    "modPlot(X_tr, y_tr, X_te, y_te, _, _, cmap = 'coolwarm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. The `__getitem__()` function in the **PyTorch Dataset** object returns \n",
    "    - A) A single observation\n",
    "    - B) As many observations as specified \n",
    "    - C) Only the index of the observation \n",
    "    - D) A dict containing a reference to the correct observation\n",
    "\n",
    "\n",
    "2. With the synthetic data that we are using in this example, what is the best choice for our loss function?\n",
    "    - A) Mean squared error\n",
    "    - B) Connectionist Temporal Classification loss\n",
    "    - C) Binary cross entropy loss\n",
    "    - D) Kullback-Leibler divergence loss\n",
    "\n",
    "\n",
    "3. How does setting the learning rate too low/high alter how our network will learn\n",
    "    - Low: we take small steps and might never hit a minima\n",
    "    - Large: bounce back and forth over the minima\n",
    "\n",
    "\n",
    "4. Which activation functions can fit our data with this configuration\n",
    "    - Tanh and ReLU fit\n",
    "    - Sigmoid does not, it may be that because the gradient of the sigmoid is smaller our weight update call is slower (setting a higher learning rate fixes this)\n",
    "\n",
    "\n",
    "5. What is a possible common issue we could run into using the sigmoid or tanh activation functions?\n",
    "    - A) Network sparsity \n",
    "    - B) Stochastic dropout\n",
    "    - C) Gradient vanishing\n",
    "    - D) Loss pooling\n",
    "    \n",
    "    \n",
    "6. How is a torch.tensor different than a standar numpy.ndarray?\n",
    "    - A) Tensor's are 64-bit complex datatype and numpy array's use 64-bit floating point\n",
    "    - B) Tensor's contain additional attributes for device casting (send to CPU/GPU) and a boolean that indicates the gradient of that tensor be calculated/stored\n",
    "    - C) Numpy arrays are better optimized for parallel computing using a CPU\n",
    "    - D) The two are equivalent, but PyTorch wanted a proprietary data object to please investors\n",
    "    \n",
    "    \n",
    "7. How many `trainable` parameters does our network have?\n",
    "    - A) 131\n",
    "    - B) 172\n",
    "    - C) 128\n",
    "    - D) 151\n",
    "\n",
    "\n",
    "8. Let's assume that our model is overfitting (not generalizing to the test data), what might we try (check all that apply)?\n",
    "    - [ ] Add regularization (i.e, weight decay to the Adam optimizer)\n",
    "    - [ ] Increase the number of layers in the network\n",
    "    - [ ] Send a lengthy email to Geoffrey Hinton and Yann LeCun\n",
    "    - [ ] Include dropout in each of our fully connected layers\n",
    "    - [ ] Use the ELU() activation function\n",
    "    - [ ] Reducing the number of parameters in our network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "- Here we begin working with convolutional neural networks\n",
    "- These powerful networks allow us to leverage spatial relationships in our N-dimensional space by using the convolution\n",
    "\n",
    "**The Convolution**\n",
    "\n",
    "$$f[x, y] * g[x, y] = \\sum_{n_{1}=-\\infty}^{\\infty} \\sum_{n_{2}=-\\infty}^{\\infty} f\\left[n_{1}, n_{2}\\right] \\cdot g\\left[x-n_{1}, y-n_{2}\\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download MNIST\n",
    "\n",
    "- This will create a `mnist` folder in your current directory and save the mnist data (`~28mb`) there\n",
    "- We will also create the dataloaders\n",
    "- The `transform` option in the datasets object allows us to \n",
    "    1. Convert our data into a `torch.tensor` \n",
    "    2. Apply some normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create folder for mnist if it doesnt exist\n",
    "if not os.path.isdir('mnist/'):\n",
    "    os.mkdir('mnist/')\n",
    "\n",
    "# The transform we will apply to both train/test\n",
    "tform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                        torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# Create the DataSet object, it will download MNIST for us\n",
    "mnist_train = torchvision.datasets.MNIST('mnist/', train=True, download=True, transform=tform)\n",
    "mnist_test = torchvision.datasets.MNIST('mnist/', train=False, download=True, transform=tform)\n",
    "\n",
    "# Create the train/test dataloaders\n",
    "mnist_train_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "mnist_test_loader = DataLoader(mnist_test, batch_size=1000, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOU CODE HERE\n",
    "\n",
    "1. Return a single observation (image, and digit) \n",
    "2. What is the shape? Does it require a gradient?\n",
    "3. Feel free to plot this single observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the observation\n",
    "_, _ = _.__getitem__(0)\n",
    "\n",
    "print(f'Shape of image: {_.shape} | MNIST Number: {_} | Requires Grad: {_._}')\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(3, 3, figsize = (8, 8))\n",
    "\n",
    "# Grab some random integers\n",
    "idx = np.random.randint(1000, size = 9)\n",
    "\n",
    "curr_idx = 0\n",
    "\n",
    "# Plot images in a grid\n",
    "for ii in [0, 1, 2]: \n",
    "    for jj in [0, 1, 2]:\n",
    "        _, _ = _.__getitem__(idx[curr_idx])\n",
    "        ax[ii, jj].imshow(_.squeeze(), cmap = 'bone')\n",
    "        curr_idx += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the convolutional neural network\n",
    "\n",
    "- Within this class we will be creating a CNN for the mnist data\n",
    "- This network will have 2 convoltional layers and 2 linear (dense) layers\n",
    "\n",
    "We are going to build our network by looking at the [PyTorch Documentation](https://pytorch.org/docs/stable/nn.html)\n",
    "\n",
    "**Specifically our network will use**\n",
    "\n",
    "* `nn.Con2d()`: 2-dimensional convolutions\n",
    "* `nn.MaxPool2d()`: 2-dimensional maxpooling (~downsampling/dim reduction along the way)\n",
    "* `nn.ReLU()`: Activation function\n",
    "* `nn.Flatten()`: To flatten our 2-dimensional data\n",
    "* `nn.Linear()`: Linear layers \n",
    "* `nn.LogSoftmax()`: Multiclass prediction probability normalization (0-1)\n",
    "\n",
    "**First Convolution Layer**\n",
    "- 2d convolution of in(1), out(2), kernel size(4)\n",
    "- 2d maxpooling (2,2)\n",
    "- Activation function\n",
    "\n",
    "**Second Convolution Layer**\n",
    "- 2d convolution of in(2), out(4), kernel size(4)\n",
    "- 2d maxpooling (2,2)\n",
    "- Activation function\n",
    "\n",
    "**Linear Layer**\n",
    "- Go from 2d to 1d\n",
    "- Linear layer of in(64), out(10)\n",
    "- Multiclass prediction probability normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOU CODE HERE\n",
    "\n",
    "class CNN_mnist(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "\n",
    "                        # Convolution layer 1\n",
    "\n",
    "\n",
    "                        # Convolution layer 2\n",
    "\n",
    "\n",
    "                        # Linear layer \n",
    "            \n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. What is the primary purpose of pooling?\n",
    "    - A) Include non-linear functions to our data\n",
    "    - B) Capture nuanced aspects of our imaging data (lines, edges, loops)\n",
    "    - C) Reduce the dimensionality\n",
    "    - D) Split kernels into similar groupings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets train the network!\n",
    "\n",
    "<font color='red'>Note</font>: this is CPU intensive, and you dont want to set your epochs to high (~3-10 should be enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Network...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train Loss: 0.5448 | Train Acc: 0.84 | Test Loss: 0.2941 | Test Acc: 0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Train Loss: 0.2800 | Train Acc: 0.94 | Test Loss: 0.2693 | Test Acc: 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Train Loss: 0.2490 | Train Acc: 0.88 | Test Loss: 0.2438 | Test Acc: 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Train Loss: 0.2329 | Train Acc: 0.94 | Test Loss: 0.2427 | Test Acc: 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Train Loss: 0.2193 | Train Acc: 0.91 | Test Loss: 0.2242 | Test Acc: 0.95\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an instance of our mnist cnn\n",
    "net = mnistCNN()\n",
    "\n",
    "# Learning rate\n",
    "lr = .001  \n",
    "\n",
    "# Number of epochs to train for\n",
    "n_epochs = 5 \n",
    "\n",
    "# Create our optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr = lr) #.01, momentum=.5)\n",
    "\n",
    "print('Initializing Network...\\n')\n",
    "\n",
    "# Store overall performance\n",
    "train_loss_tot = []\n",
    "test_loss_tot = []\n",
    "\n",
    "train_acc_tot = []\n",
    "test_acc_tot = []\n",
    "\n",
    "for epoch in range(5):\n",
    "    train_loss_arr = []\n",
    "    test_loss_arr = []\n",
    "    \n",
    "    # Training set and weight updating\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(mnist_train_loader, leave = False)):\n",
    "            \n",
    "            # Zero the gradient\n",
    "            optimizer.zero_grad() \n",
    "            \n",
    "            # Forward pass\n",
    "            output = net.model(data)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = F.nll_loss(output, target)\n",
    "            \n",
    "            # Backprop and step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Store loss and accuracy\n",
    "            train_loss_arr.append(loss.item())\n",
    "            train_acc_tot.append((output.argmax(dim = 1) == target).sum() / target.shape[0])\n",
    "    \n",
    "    # Test evaluation\n",
    "    with torch.no_grad():\n",
    "        for test_batch_idx, (test_data, test_target) in enumerate(mnist_test_loader):\n",
    "            \n",
    "            # Forward pass (get prediction)\n",
    "            test_output = net.model(test_data)\n",
    "            \n",
    "            # Compute loss\n",
    "            test_loss = F.nll_loss(test_output, test_target)\n",
    "            \n",
    "            # Store loss and accuracy\n",
    "            test_loss_arr.append(test_loss.item())\n",
    "            test_acc_tot.append((test_output.argmax(dim = 1) == test_target).sum() / test_target.shape[0])\n",
    "\n",
    "            \n",
    "    train_loss_tot.append(np.mean(np.array(train_loss_arr)))\n",
    "    test_loss_tot.append(np.mean(np.array(test_loss_arr)))\n",
    "    \n",
    "    print(f'Epoch: {epoch+1} | Train Loss: {train_loss_tot[-1]:.4f} | Train Acc: {train_acc_tot[-1]:.2f} | Test Loss: {test_loss_tot[-1]:.4f} | Test Acc: {test_acc_tot[-1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a fully connected layer for the network\n",
    "\n",
    "- Lets compare the performance of the CNN against a network with no convolutions\n",
    "\n",
    "**Network Structure**\n",
    "\n",
    "1. Data needs to be 1D (need to flatten)\n",
    "2. linear layer with (in = our data dim flattened, out = 10)\n",
    "3. ReLU activation\n",
    "4. linear layer with (in = 10, out = 10)\n",
    "5. ReLU activation\n",
    "6. linear layer with (in = 10, out = 10)\n",
    "7. LogSoftMax \n",
    "\n",
    "**Now rerun with the dense network below**\n",
    "\n",
    "\n",
    "**How does this network compare with our CNN?**\n",
    "\n",
    "10. What might be happening and why is the convolutional neural network acheiving higher test accuracy?\n",
    "    - [ ] The added weight decay allows us to uncover variability in pixel intensity\n",
    "    - [ ] The network without convolutions has too many layers and is suffering from gradient vanishing\n",
    "    - [ ] The CNN allows us to uncover low level features and relationships among these \n",
    "    - [ ] The CNN has substantially fewer trainable parameters and may be able to generalize better\n",
    "    - [ ] Flattening our 2D images obscures import context embedded in that native 2D space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOU CODE HERE\n",
    "\n",
    "class mnistDense(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            \n",
    "                            # Layer 1\n",
    "\n",
    "\n",
    "                            # Layer 2\n",
    "\n",
    "            \n",
    "                            # Layer 3\n",
    "\n",
    "                                   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an instance of our mnist cnn\n",
    "net = mnistDense()\n",
    "\n",
    "# Learning rate\n",
    "lr = .001  \n",
    "\n",
    "# Number of epochs to train for\n",
    "n_epochs = 5 \n",
    "\n",
    "# Create our optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr = lr) #.01, momentum=.5)\n",
    "\n",
    "print('Initializing Network...\\n')\n",
    "\n",
    "# Store overall performance\n",
    "train_loss_tot = []\n",
    "test_loss_tot = []\n",
    "\n",
    "train_acc_tot = []\n",
    "test_acc_tot = []\n",
    "\n",
    "for epoch in range(5):\n",
    "    train_loss_arr = []\n",
    "    test_loss_arr = []\n",
    "    \n",
    "    # Training set and weight updating\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(mnist_train_loader, leave = False)):\n",
    "            \n",
    "            # Zero the gradient\n",
    "            optimizer.zero_grad() \n",
    "            \n",
    "            # Forward pass\n",
    "            output = net.model(data)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = F.nll_loss(output, target)\n",
    "            \n",
    "            # Backprop and step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Store loss and accuracy\n",
    "            train_loss_arr.append(loss.item())\n",
    "            train_acc_tot.append((output.argmax(dim = 1) == target).sum() / target.shape[0])\n",
    "    \n",
    "    # Test evaluation\n",
    "    with torch.no_grad():\n",
    "        for test_batch_idx, (test_data, test_target) in enumerate(mnist_test_loader):\n",
    "            \n",
    "            # Forward pass (get prediction)\n",
    "            test_output = net.model(test_data)\n",
    "            \n",
    "            # Compute loss\n",
    "            test_loss = F.nll_loss(test_output, test_target)\n",
    "            \n",
    "            # Store loss and accuracy\n",
    "            test_loss_arr.append(test_loss.item())\n",
    "            test_acc_tot.append((test_output.argmax(dim = 1) == test_target).sum() / test_target.shape[0])\n",
    "\n",
    "            \n",
    "    train_loss_tot.append(np.mean(np.array(train_loss_arr)))\n",
    "    test_loss_tot.append(np.mean(np.array(test_loss_arr)))\n",
    "    \n",
    "    print(f'Epoch: {epoch+1} | Train Loss: {train_loss_tot[-1]:.4f} | Train Acc: {train_acc_tot[-1]:.2f} | Test Loss: {test_loss_tot[-1]:.4f} | Test Acc: {test_acc_tot[-1]:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
